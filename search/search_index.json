{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The English SDK for Apache Spark is an extremely simple yet powerful tool. It takes English instructions and compile them into PySpark objects like DataFrames. Its goal is to make Spark more user-friendly and accessible, allowing you to focus your efforts on extracting insights from your data.</p> <p></p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#dataframe-transformation","title":"DataFrame Transformation","text":"<p>Given the following DataFrame <code>df</code>, you can write English to transform it to another DataFrame. For example: <pre><code>df.ai.transform(\"What are the best-selling and the second best-selling products in every category?\").show()\n</code></pre></p> product category revenue Foldable Cellphone 6500 Nromal Cellphone 6000 Mini Tablet 5500 Pro Tablet 4000"},{"location":"#data-ingestion","title":"Data Ingestion","text":"<pre><code>auto_df = spark_ai.create_df(\"2022 USA national auto sales by brand\")\n</code></pre>"},{"location":"#plot","title":"Plot","text":"<p><pre><code>auto_df.ai.plot(\"pie chart for US sales market shares, show the top 5 brands and the sum of others\")\n</code></pre> </p>"},{"location":"cache/","title":"Cache","text":"<p>The English SDK supports a simple in-memory and persistent cache system. It keeps an in-memory staging cache, which gets updated for LLM and web search results. The staging cache can be persisted through the commit() method. Cache lookup is always performed on both in-memory staging cache and persistent cache. <pre><code>spark_ai.commit()\n</code></pre></p>"},{"location":"contributing/","title":"Contributing","text":"<p>We're delighted that you're considering contributing to the English SDK for Apache Spark project! Whether you're fixing a bug or proposing a new feature, your contribution is highly appreciated.</p> <p>Before you start, please take a moment to read our Contribution Guide. This guide provides an overview of how you can contribute to our project. We're currently in the early stages of development and we're working on introducing more comprehensive test cases and Github Action jobs for enhanced testing of each pull request.</p> <p>If you have any questions or need assistance, feel free to open a new issue in the GitHub repository.</p> <p>Thank you for helping us improve the English SDK for Apache Spark. We're excited to see your contributions!</p>"},{"location":"data_ingestion/","title":"Data Ingestion","text":""},{"location":"data_ingestion/#api","title":"API","text":"<pre><code>spark_ai.create_df(\n    desc: str,\n    columns: Optional[List[str]] = None,\n    cache: bool = True) -&gt; DataFrame\n</code></pre> <p>Given a SparkAI instance <code>spark_ai</code>, you can use this method to create a Spark DataFrame by querying an LLM from web search result.</p> <ul> <li>param desc: the description of the result DataFrame, which will be used for web searching</li> <li>param columns: the expected column names in the result DataFrame</li> <li>param cache: If <code>True</code>, fetches cached data, if available. If <code>False</code>, retrieves fresh data and updates cache.</li> <li>return: a Spark DataFrame</li> </ul>"},{"location":"data_ingestion/#example","title":"Example","text":"<p>If you have set up the Google Python client, you can ingest data via search engine: <pre><code>auto_df = spark_ai.create_df(\"2022 USA national auto sales by brand\")\n</code></pre> Otherwise, you can ingest data via URL: <pre><code>auto_df = spark_ai.create_df(\"https://www.carpro.com/blog/full-year-2022-national-auto-sales-by-brand\")\n</code></pre></p> <p>Take a look at the data: <pre><code>auto_df.show(n=5)\n</code></pre></p> rank brand us_sales_2022 sales_change_vs_2021 1 Toyota 1849751 -9 2 Ford 1767439 -2 3 Chevrolet 1502389 6 4 Honda 881201 -33 5 Hyundai 724265 -2"},{"location":"dataframe_explanation/","title":"DataFrame Explanation","text":""},{"location":"dataframe_explanation/#api","title":"API","text":"<pre><code>DataFrame.ai.explain(cache: bool = True) -&gt; str:\n</code></pre> <p>This method generates a natural language explanation of the SQL plan of the input Spark DataFrame.</p>"},{"location":"dataframe_explanation/#example","title":"Example","text":"<p>Given a DataFrame <code>auto_df</code> from Data Ingestion, you can explain a DataFrame with the following code: <pre><code>auto_top_growth_df=auto_df.ai.transform(\"brand with the highest growth\")\nauto_top_growth_df.ai.explain()\n</code></pre></p> <p>In summary, this dataframe is retrieving the brand with the highest sales change in 2022 compared to 2021. It presents the results sorted by sales change in descending order and only returns the top result.</p>"},{"location":"dataframe_transformation/","title":"DataFrame Transformation","text":""},{"location":"dataframe_transformation/#api","title":"API","text":"<pre><code>DataFrame.ai.transform(desc: str, cache: bool = True) -&gt; DataFrame\n</code></pre> <p>This method applies a transformation to a provided Spark DataFrame, the specifics of which are determined by the <code>desc</code> parameter:</p> <ul> <li>param desc: A natural language string that outlines the specific transformation to be applied on the DataFrame.</li> <li>param cache: If <code>True</code>, fetches cached data, if available. If <code>False</code>, retrieves fresh data and updates cache.</li> <li>return: Returns a new Spark DataFrame that is the result of applying the specified transformation                  on the input DataFrame.</li> </ul>"},{"location":"dataframe_transformation/#example","title":"Example","text":"<p>Given the following DataFrame <code>df</code>: <pre><code>df = spark_ai._spark.createDataFrame(\n    [\n        (\"Normal\", \"Cellphone\", 6000),\n        (\"Normal\", \"Tablet\", 1500),\n        (\"Mini\", \"Tablet\", 5500),\n        (\"Mini\", \"Cellphone\", 5000),\n        (\"Foldable\", \"Cellphone\", 6500),\n        (\"Foldable\", \"Tablet\", 2500),\n        (\"Pro\", \"Cellphone\", 3000),\n        (\"Pro\", \"Tablet\", 4000),\n        (\"Pro Max\", \"Cellphone\", 4500)\n    ],\n    [\"product\", \"category\", \"revenue\"]\n)\n</code></pre></p> <p>You can write English to perform transformations. For example: <pre><code>df.ai.transform(\"What are the best-selling and the second best-selling products in every category?\").show()\n</code></pre></p> product category revenue Foldable Cellphone 6500 Nromal Cellphone 6000 Mini Tablet 5500 Pro Tablet 4000 <pre><code>df.ai.transform(\"Pivot the data by product and the revenue for each product\").show()\n</code></pre> Category Normal Mini Foldable Pro Pro Max Cellphone 6000 5000 6500 3000 4500 Tablet 1500 5500 2500 4000 null <p>For a detailed walkthrough of the transformations, please refer to our transform_dataframe.ipynb notebook.</p>"},{"location":"dataframe_verification/","title":"DataFrame Verification","text":""},{"location":"dataframe_verification/#api","title":"API","text":"<p><pre><code>DataFrame.ai.verify(desc: Optional[str] = None, cache: bool = True) -&gt; bool\n</code></pre> This method creates and runs test cases for the provided PySpark dataframe transformation function. The result is shown in the logging output. The mthod returns <code>True</code> if tranformation is valid, otherwise it returns <code>False</code> and logs errors.</p>"},{"location":"dataframe_verification/#example","title":"Example","text":"<p>Given a DataFrame <code>auto_df</code> from Data Ingestion: <pre><code>auto_df.ai.verify(\"expect sales change percentage to be between -100 to 100\")\n</code></pre></p> <p>result: True</p>"},{"location":"installation_and_setup/","title":"Installation and Setup","text":""},{"location":"installation_and_setup/#installation","title":"Installation","text":"<p>pyspark-ai can be installed via pip from PyPI: <pre><code>pip install pyspark-ai\n</code></pre></p> <p>pyspark-ai can also be installed with optional dependencies to enable certain functionality.  For example, to install pyspark-ai with the optional dependencies to plot data from a DataFrame:</p> <pre><code>pip install \"pyspark-ai[plot]\"\n</code></pre> <p>For a full list of optional dependencies, see the Optional Dependencies section.</p>"},{"location":"installation_and_setup/#configuring-openai-llms","title":"Configuring OpenAI LLMs","text":"<p>As of July 2023, we have found that the GPT-4 works optimally with the English SDK. This superior AI model is readily accessible to all developers through the OpenAI API.</p> <p>To use OpenAI's Language Learning Models (LLMs), you can set your OpenAI secret key as the <code>OPENAI_API_KEY</code> environment variable. This key can be found in your OpenAI account: <pre><code>export OPENAI_API_KEY='sk-...'\n</code></pre> By default, the <code>SparkAI</code> instances will use the GPT-4 model. However, you're encouraged to experiment with creating and implementing other LLMs, which can be passed during the initialization of <code>SparkAI</code> instances for various use-cases.</p>"},{"location":"installation_and_setup/#initialization","title":"Initialization","text":"<pre><code>from pyspark_ai import SparkAI\n\nspark_ai = SparkAI()\nspark_ai.activate()  # active partial functions for Spark DataFrame\n</code></pre> <p>You can also pass other LLMs to construct the SparkAI instance. For example, by following this guide: <pre><code>from langchain.chat_models import AzureChatOpenAI\nfrom pyspark_ai import SparkAI\n\nllm = AzureChatOpenAI(\n    deployment_name=...,\n    model_name=...\n)\nspark_ai = SparkAI(llm=llm)\nspark_ai.activate()  # active partial functions for Spark DataFrame\n</code></pre></p> <p>As per Microsoft's Data Privacy page, using the Azure OpenAI service can provide better data privacy and security.</p>"},{"location":"installation_and_setup/#optional-dependencies","title":"Optional Dependencies","text":"<p>pyspark-ai has many optional dependencies that are only used for specific methods.  For example, ingestion via <code>spark_ai.create_df(\"...\")</code> requires the <code>requests</code> package, while plotting via <code>df.plot()</code> requires the <code>plotly</code> package.  If the optional dependency is not installed, pyspark-ai will raise an Exception if a method requiring that dependency is called.</p> <p>If using pip, optional pyspark-ai dependencies can be installed as optional extras, e.g. <code>pip install \"pyspark-ai[ingestion, plot]\"</code>.  All optional dependencies can be installed with <code>pip install \"pyspark-ai[all]\"</code>.</p> <p>Specific groups and their associated dependencies are listed below. For more details about groups, see the README.md.</p> Group Description Dependencies Pip Installation Plot Generate visualizations for DataFrame pandas, plotly <code>pip install \"pyspark-ai[plot]\"</code> Vector Search Improve query generation accuracy in transformations faiss-cpu, sentence-transformers, torch <code>pip install \"pyspark-ai[vector-search]\"</code> Ingestion Ingest data into a DataFrame, from URLs or descriptions requests, tiktoken, beautifulsoup4, google-api-python-client <code>pip install \"pyspark-ai[ingestion]\"</code> Spark Connect Support Spark Connect grpcio, grpcio-status, pyarrow <code>pip install \"pyspark-ai[spark-connect]\"</code>"},{"location":"plot/","title":"Plot","text":""},{"location":"plot/#api","title":"API","text":"<p><pre><code>DataFrame.ai.plot(desc: Optional[str] = None, cache: bool = True) -&gt; str\n</code></pre> This method is used to plot a Spark DataFrame, the specifics of which are determined by the <code>desc</code> parameter. If <code>desc</code> is not provided, the method will try to plot the DataFrame based on its schema.</p> <ul> <li>param desc: An optional natural language string that outlines the specific transformation to be applied on the DataFrame.</li> <li>param cache: If <code>True</code>, fetches cached data, if available. If <code>False</code>, retrieves fresh data and updates cache.</li> <li>return: Returns the generated code as a string. If the generated code is not valid Python code, an empty string is returned.</li> </ul>"},{"location":"plot/#example","title":"Example","text":"<p>Given a DataFrame <code>auto_df</code> from Data Ingestion, you can plot it with the following code: <pre><code>auto_df.ai.plot()\n</code></pre> </p> <p>To plot with an instruction: <pre><code>auto_df.ai.plot(\"pie chart for US sales market shares, show the top 5 brands and the sum of others\")\n</code></pre> </p>"},{"location":"resources/","title":"Resources","text":"<p>For a more comprehensive introduction and background to our project, we have the following resources:</p> <ul> <li>Blog Post: A detailed walkthrough of our project.</li> <li>Demo Video: 2023 Data + AI summit announcement video with demo.</li> <li>Breakout Session: A deep dive into the story behind the English SDK, its features, and future works at DATA+AI summit 2023.</li> </ul>"},{"location":"udf_generation/","title":"UDF Generation","text":""},{"location":"udf_generation/#api","title":"API","text":"<p><pre><code>@spark_ai.udf\ndef udf_name(arg1: arg1_type, arg2: arg2_type, ...) -&gt; return_type:\n    \"\"\"UDF description\"\"\"\n    ...\n</code></pre> Given a SparkAI instance <code>spark_ai</code>, you can use the <code>@spark_ai.udf</code> decorator to generate UDFs from Python functions. There is no need to implement the body of the method.</p>"},{"location":"udf_generation/#example-1-compute-expression-from-columns","title":"Example 1: Compute expression from columns","text":"<p>Given a DataFrame <code>auto_df</code> from Data Ingestion: <pre><code>@spark_ai.udf\ndef previous_years_sales(brand: str, current_year_sale: int, sales_change_percentage: float) -&gt; int:\n    \"\"\"Calculate previous years sales from sales change percentage\"\"\"\n    ...\n\nspark.udf.register(\"previous_years_sales\", previous_years_sales)\nauto_df.createOrReplaceTempView(\"autoDF\")\n\nspark.sql(\"select brand as brand, previous_years_sales(brand, us_sales, sales_change_percentage) as 2021_sales from autoDF\").show()\n</code></pre></p> brand 2021_sales Toyota 2032693 Ford 1803509 Chevrolet 1417348 Honda 1315225 Hyundai 739045"},{"location":"udf_generation/#example-2-parse-heterogeneous-json-text","title":"Example 2: Parse heterogeneous JSON text","text":"<p>Let's imagine we have heterogeneous JSON texts: each of them may contain or not keys and also order of keys is random. We can generate sucha a DataFrame by mutating single JSON.</p> <pre><code>random_dict = {\n    \"id\": 1279,\n    \"first_name\": \"John\",\n    \"last_name\": \"Doe\",\n    \"username\": \"johndoe\",\n    \"email\": \"john_doe@example.com\",\n    \"phone_number\": \"+1 234 567 8900\",\n    \"address\": \"123 Main St, Springfield, OH, 45503, USA\",\n    \"age\": 32,\n    \"registration_date\": \"2020-01-20T12:12:12Z\",\n    \"last_login\": \"2022-03-21T07:25:34Z\",\n}\noriginal_keys = list(random_dict.keys())\n\nfrom random import random, shuffle\n\nmutaded_rows = []\nfor _ in range(20):\n    keys = [k for k in original_keys]\n    shuffle(keys)\n    # With 0.4 chance drop each field and also shuffle an order\n    mutaded_rows.append({k: random_dict[k] for k in keys if random() &lt;= 0.6})\n\nimport json\n\nbad_json_dataframe = (\n    spark.createDataFrame(\n        [(json.dumps(val), original_keys) for val in mutaded_rows],\n        [\"json_field\", \"schema\"],\n    )\n)\n</code></pre> json_field schema {\"first_name\": \"John\", \"email\": \"john_doe@example.com\", \"last_name\": \"Doe\", \"phone_number\": \"+1 234 567 8900\", \"age\": 32, \"last_login\": \"2022-03-21T07:25:34Z\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] {\"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"phone_number\": \"+1 234 567 8900\", \"email\": \"john_doe@example.com\", \"registration_date\": \"2020-01-20T12:12:12Z\", \"username\": \"johndoe\", \"last_login\": \"2022-03-21T07:25:34Z\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] {\"age\": 32, \"last_name\": \"Doe\", \"email\": \"john_doe@example.com\", \"last_login\": \"2022-03-21T07:25:34Z\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"username\": \"johndoe\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] {\"first_name\": \"John\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"phone_number\": \"+1 234 567 8900\", \"last_name\": \"Doe\", \"id\": 1279} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] {\"phone_number\": \"+1 234 567 8900\", \"registration_date\": \"2020-01-20T12:12:12Z\", \"email\": \"john_doe@example.com\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"age\": 32, \"username\": \"johndoe\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] <pre><code>from typing import List\n\n@spark_ai.udf\ndef parse_heterogeneous_json(json_str: str, schema: List[str]) -&gt; List[str]:\n    \"\"\"Extract fields from heterogeneous JSON string based on given schema in a right order.\n    If field is missing replace it by None. All imports should be inside function.\"\"\"\n    ...\n</code></pre> <p>Now we can test processing of our text rows:</p> <pre><code>from pyspark.sql.functions import expr\n\n(\n    bad_json_dataframe\n    .withColumn(\n        \"parsed\",\n        expr(\"parse_heterogeneous_json(json_field, schema)\"),\n    )\n    .select(\"parsed\")\n    .show()\n</code></pre> json_field schema parsed {\"first_name\": \"John\", \"email\": \"john_doe@example.com\", \"last_name\": \"Doe\", \"phone_number\": \"+1 234 567 8900\", \"age\": 32, \"last_login\": \"2022-03-21T07:25:34Z\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] [None, John, Doe, None, john_doe@example.com, +1 234 567 8900, 123 Main St, Springfield, OH, 45503, USA, 32, None, 2022-03-21T07:25:34Z] {\"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"phone_number\": \"+1 234 567 8900\", \"email\": \"john_doe@example.com\", \"registration_date\": \"2020-01-20T12:12:12Z\", \"username\": \"johndoe\", \"last_login\": \"2022-03-21T07:25:34Z\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] [None, None, None, johndoe, john_doe@example.com, +1 234 567 8900, 123 Main St, Springfield, OH, 45503, USA, None, 2020-01-20T12:12:12Z, 2022-03-21T07:25:34Z] {\"age\": 32, \"last_name\": \"Doe\", \"email\": \"john_doe@example.com\", \"last_login\": \"2022-03-21T07:25:34Z\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"username\": \"johndoe\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] [None, None, Doe, johndoe, john_doe@example.com, None, 123 Main St, Springfield, OH, 45503, USA, 32, None, 2022-03-21T07:25:34Z] {\"first_name\": \"John\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"phone_number\": \"+1 234 567 8900\", \"last_name\": \"Doe\", \"id\": 1279} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] [1279, John, Doe, None, None, +1 234 567 8900, 123 Main St, Springfield, OH, 45503, USA, None, None, None] {\"phone_number\": \"+1 234 567 8900\", \"registration_date\": \"2020-01-20T12:12:12Z\", \"email\": \"john_doe@example.com\", \"address\": \"123 Main St, Springfield, OH, 45503, USA\", \"age\": 32, \"username\": \"johndoe\"} [id, first_name, last_name, username, email, phone_number, address, age, registration_date, last_login] [None, None, None, johndoe, john_doe@example.com, +1 234 567 8900, 123 Main St, Springfield, OH, 45503, USA, 32, 2020-01-20T12:12:12Z, None]"},{"location":"udf_generation/#example-3-extract-email-from-raw-text","title":"Example 3: Extract email from raw text","text":"<pre><code>df = spark.createDataFrame(\n    [\n        \"For any queries regarding the product, contact helpdesk@example.com.\",\n        \"Send your applications to hr@ourcompany.com.\",\n        \"You can reach out to the teacher at prof.mike@example.edu.\",\n        \"My personal email is jane.doe@example.com.\",\n        \"You can forward the documents to admin@oursite.net.\",\n    ],\n    schema=\"string\",\n)\n</code></pre> <pre><code>@spark_ai.udf\ndef extract_email(text: str) -&gt; str:\n    \"\"\"Extract first email from raw text\"\"\"\n    ...\n\nfrom pyspark.sql.functions import col\n\nspark.udf.register(\"extract_email\", extract_email)\ndf.select(col(\"value\").alias(\"raw\"), expr(\"extract_email(value)\").alias(\"email\")).show()\n</code></pre> raw email For any queries regarding the product, contact helpdesk@example.com. helpdesk@example.com Send your applications to hr@ourcompany.com. hr@ourcompany.com You can reach out to the teacher at prof.mike@example.edu. prof.mike@example.edu My personal email is jane.doe@example.com. jane.doe@example.com You can forward the documents to admin@oursite.net. admin@oursite.net"},{"location":"udf_generation/#example-4-generate-random-numbers-from-laplace-distribution","title":"Example 4: Generate random numbers from Laplace distribution","text":"<pre><code>@spark_ai.udf\ndef laplace_random_number(loc: float, scale: float) -&gt; float:\n    \"\"\"Generate a random number from Laplace distribution with given loc and scale in pure Python. Function should contain all necessary imports.\"\"\"\n    ...\n</code></pre> <pre><code>from pyspark.sql.functions import lit\nfrom pyspark.sql.types import DoubleType\n\nspark.udf.register(\"laplace_random_number\", laplace_random_number, returnType=DoubleType())\n(\n    spark.sparkContext.range(0, 500_000)\n    .toDF(schema=\"int\")\n    .withColumn(\"loc\", lit(1.0).cast(\"double\"))\n    .withColumn(\"scale\", lit(0.3).cast(\"double\"))\n    .withColumn(\"laplace_random\", expr(\"laplace_random_number(loc, scale)\"))\n    .select(\"laplace_random\")\n    .show()\n)\n</code></pre> value loc scale laplace_random 0 1.0 0.3 0.799962 1 1.0 0.3 0.995381 2 1.0 0.3 0.602727 3 1.0 0.3 1.235575 4 1.0 0.3 1.864565 5 1.0 0.3 1.220493 6 1.0 0.3 0.992431 7 1.0 0.3 1.630307 8 1.0 0.3 0.894683 9 1.0 0.3 0.632602"}]}